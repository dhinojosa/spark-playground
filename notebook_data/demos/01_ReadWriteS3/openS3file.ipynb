{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "65017f32-be8d-4194-95fc-e5d2534ba8f1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ":: loading settings :: url = jar:file:/opt/bitnami/spark/jars/ivy-2.5.0.jar!/org/apache/ivy/core/settings/ivysettings.xml\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Ivy Default Cache set to: /root/.ivy2/cache\n",
      "The jars for the packages stored in: /root/.ivy2/jars\n",
      "org.apache.hadoop#hadoop-aws added as a dependency\n",
      "com.amazonaws#aws-java-sdk-bundle added as a dependency\n",
      "org.apache.spark#spark-hadoop-cloud_2.12 added as a dependency\n",
      ":: resolving dependencies :: org.apache.spark#spark-submit-parent-8ec415c2-5ee3-43dc-9788-b8b1d74aedb9;1.0\n",
      "\tconfs: [default]\n",
      "\tfound org.apache.hadoop#hadoop-aws;3.2.0 in central\n",
      "\tfound com.amazonaws#aws-java-sdk-bundle;1.11.704 in central\n",
      "\tfound org.apache.spark#spark-hadoop-cloud_2.12;3.3.0 in central\n",
      "\tfound org.apache.hadoop#hadoop-client-runtime;3.3.2 in central\n",
      "\tfound org.apache.hadoop#hadoop-client-api;3.3.2 in central\n",
      "\tfound org.xerial.snappy#snappy-java;1.1.8.4 in central\n",
      "\tfound org.slf4j#slf4j-api;1.7.32 in central\n",
      "\tfound commons-logging#commons-logging;1.1.3 in central\n",
      "\tfound com.google.code.findbugs#jsr305;3.0.0 in central\n",
      "\tfound org.apache.hadoop#hadoop-aws;3.3.2 in central\n",
      "\tfound com.amazonaws#aws-java-sdk-bundle;1.11.1026 in central\n",
      "\tfound org.wildfly.openssl#wildfly-openssl;1.0.7.Final in central\n",
      "\tfound org.apache.hadoop#hadoop-openstack;3.3.2 in central\n",
      "\tfound org.apache.hadoop#hadoop-annotations;3.3.2 in central\n",
      "\tfound org.apache.httpcomponents#httpcore;4.4.14 in central\n",
      "\tfound com.fasterxml.jackson.core#jackson-annotations;2.13.3 in central\n",
      "\tfound com.fasterxml.jackson.core#jackson-databind;2.13.3 in central\n",
      "\tfound com.fasterxml.jackson.core#jackson-core;2.13.3 in central\n",
      "\tfound joda-time#joda-time;2.10.13 in central\n",
      "\tfound com.fasterxml.jackson.dataformat#jackson-dataformat-cbor;2.13.3 in central\n",
      "\tfound org.apache.httpcomponents#httpclient;4.5.13 in central\n",
      "\tfound commons-codec#commons-codec;1.15 in central\n",
      "\tfound org.apache.hadoop#hadoop-azure;3.3.2 in central\n",
      "\tfound com.microsoft.azure#azure-storage;7.0.1 in central\n",
      "\tfound com.microsoft.azure#azure-keyvault-core;1.0.0 in central\n",
      "\tfound org.apache.hadoop.thirdparty#hadoop-shaded-guava;1.1.1 in central\n",
      "\tfound org.codehaus.jackson#jackson-mapper-asl;1.9.13 in central\n",
      "\tfound org.codehaus.jackson#jackson-core-asl;1.9.13 in central\n",
      "\tfound org.apache.hadoop#hadoop-cloud-storage;3.3.2 in central\n",
      "\tfound org.apache.hadoop#hadoop-aliyun;3.3.2 in central\n",
      "\tfound com.aliyun.oss#aliyun-sdk-oss;3.13.0 in central\n",
      "\tfound org.jdom#jdom2;2.0.6 in central\n",
      "\tfound org.codehaus.jettison#jettison;1.1 in central\n",
      "\tfound stax#stax-api;1.0.1 in central\n",
      "\tfound com.aliyun#aliyun-java-sdk-core;4.5.10 in central\n",
      "\tfound com.google.code.gson#gson;2.8.9 in central\n",
      "\tfound javax.xml.bind#jaxb-api;2.2.11 in central\n",
      "\tfound org.ini4j#ini4j;0.5.4 in central\n",
      "\tfound io.opentracing#opentracing-api;0.33.0 in central\n",
      "\tfound io.opentracing#opentracing-util;0.33.0 in central\n",
      "\tfound io.opentracing#opentracing-noop;0.33.0 in central\n",
      "\tfound com.aliyun#aliyun-java-sdk-ram;3.1.0 in central\n",
      "\tfound com.aliyun#aliyun-java-sdk-kms;2.11.0 in central\n",
      "\tfound org.apache.hadoop#hadoop-azure-datalake;3.3.2 in central\n",
      "\tfound com.microsoft.azure#azure-data-lake-store-sdk;2.3.9 in central\n",
      "\tfound org.apache.hadoop#hadoop-cos;3.3.2 in central\n",
      "\tfound com.qcloud#cos_api-bundle;5.6.19 in central\n",
      "\tfound org.eclipse.jetty#jetty-util-ajax;9.4.46.v20220331 in central\n",
      "\tfound org.eclipse.jetty#jetty-util;9.4.46.v20220331 in central\n",
      "\tfound org.spark-project.spark#unused;1.0.0 in central\n",
      "downloading https://repo1.maven.org/maven2/stax/stax-api/1.0.1/stax-api-1.0.1.jar ...\n",
      "\t[SUCCESSFUL ] stax#stax-api;1.0.1!stax-api.jar (99ms)\n",
      ":: resolution report :: resolve 5070ms :: artifacts dl 203ms\n",
      "\t:: modules in use:\n",
      "\tcom.aliyun#aliyun-java-sdk-core;4.5.10 from central in [default]\n",
      "\tcom.aliyun#aliyun-java-sdk-kms;2.11.0 from central in [default]\n",
      "\tcom.aliyun#aliyun-java-sdk-ram;3.1.0 from central in [default]\n",
      "\tcom.aliyun.oss#aliyun-sdk-oss;3.13.0 from central in [default]\n",
      "\tcom.amazonaws#aws-java-sdk-bundle;1.11.1026 from central in [default]\n",
      "\tcom.fasterxml.jackson.core#jackson-annotations;2.13.3 from central in [default]\n",
      "\tcom.fasterxml.jackson.core#jackson-core;2.13.3 from central in [default]\n",
      "\tcom.fasterxml.jackson.core#jackson-databind;2.13.3 from central in [default]\n",
      "\tcom.fasterxml.jackson.dataformat#jackson-dataformat-cbor;2.13.3 from central in [default]\n",
      "\tcom.google.code.findbugs#jsr305;3.0.0 from central in [default]\n",
      "\tcom.google.code.gson#gson;2.8.9 from central in [default]\n",
      "\tcom.microsoft.azure#azure-data-lake-store-sdk;2.3.9 from central in [default]\n",
      "\tcom.microsoft.azure#azure-keyvault-core;1.0.0 from central in [default]\n",
      "\tcom.microsoft.azure#azure-storage;7.0.1 from central in [default]\n",
      "\tcom.qcloud#cos_api-bundle;5.6.19 from central in [default]\n",
      "\tcommons-codec#commons-codec;1.15 from central in [default]\n",
      "\tcommons-logging#commons-logging;1.1.3 from central in [default]\n",
      "\tio.opentracing#opentracing-api;0.33.0 from central in [default]\n",
      "\tio.opentracing#opentracing-noop;0.33.0 from central in [default]\n",
      "\tio.opentracing#opentracing-util;0.33.0 from central in [default]\n",
      "\tjavax.xml.bind#jaxb-api;2.2.11 from central in [default]\n",
      "\tjoda-time#joda-time;2.10.13 from central in [default]\n",
      "\torg.apache.hadoop#hadoop-aliyun;3.3.2 from central in [default]\n",
      "\torg.apache.hadoop#hadoop-annotations;3.3.2 from central in [default]\n",
      "\torg.apache.hadoop#hadoop-aws;3.3.2 from central in [default]\n",
      "\torg.apache.hadoop#hadoop-azure;3.3.2 from central in [default]\n",
      "\torg.apache.hadoop#hadoop-azure-datalake;3.3.2 from central in [default]\n",
      "\torg.apache.hadoop#hadoop-client-api;3.3.2 from central in [default]\n",
      "\torg.apache.hadoop#hadoop-client-runtime;3.3.2 from central in [default]\n",
      "\torg.apache.hadoop#hadoop-cloud-storage;3.3.2 from central in [default]\n",
      "\torg.apache.hadoop#hadoop-cos;3.3.2 from central in [default]\n",
      "\torg.apache.hadoop#hadoop-openstack;3.3.2 from central in [default]\n",
      "\torg.apache.hadoop.thirdparty#hadoop-shaded-guava;1.1.1 from central in [default]\n",
      "\torg.apache.httpcomponents#httpclient;4.5.13 from central in [default]\n",
      "\torg.apache.httpcomponents#httpcore;4.4.14 from central in [default]\n",
      "\torg.apache.spark#spark-hadoop-cloud_2.12;3.3.0 from central in [default]\n",
      "\torg.codehaus.jackson#jackson-core-asl;1.9.13 from central in [default]\n",
      "\torg.codehaus.jackson#jackson-mapper-asl;1.9.13 from central in [default]\n",
      "\torg.codehaus.jettison#jettison;1.1 from central in [default]\n",
      "\torg.eclipse.jetty#jetty-util;9.4.46.v20220331 from central in [default]\n",
      "\torg.eclipse.jetty#jetty-util-ajax;9.4.46.v20220331 from central in [default]\n",
      "\torg.ini4j#ini4j;0.5.4 from central in [default]\n",
      "\torg.jdom#jdom2;2.0.6 from central in [default]\n",
      "\torg.slf4j#slf4j-api;1.7.32 from central in [default]\n",
      "\torg.spark-project.spark#unused;1.0.0 from central in [default]\n",
      "\torg.wildfly.openssl#wildfly-openssl;1.0.7.Final from central in [default]\n",
      "\torg.xerial.snappy#snappy-java;1.1.8.4 from central in [default]\n",
      "\tstax#stax-api;1.0.1 from central in [default]\n",
      "\t:: evicted modules:\n",
      "\torg.apache.hadoop#hadoop-aws;3.2.0 by [org.apache.hadoop#hadoop-aws;3.3.2] in [default]\n",
      "\tcom.amazonaws#aws-java-sdk-bundle;1.11.375 by [com.amazonaws#aws-java-sdk-bundle;1.11.704] in [default]\n",
      "\tcom.amazonaws#aws-java-sdk-bundle;1.11.704 by [com.amazonaws#aws-java-sdk-bundle;1.11.1026] in [default]\n",
      "\torg.eclipse.jetty#jetty-util-ajax;9.4.43.v20210629 by [org.eclipse.jetty#jetty-util-ajax;9.4.46.v20220331] in [default]\n",
      "\t---------------------------------------------------------------------\n",
      "\t|                  |            modules            ||   artifacts   |\n",
      "\t|       conf       | number| search|dwnlded|evicted|| number|dwnlded|\n",
      "\t---------------------------------------------------------------------\n",
      "\t|      default     |   52  |   1   |   1   |   4   ||   48  |   1   |\n",
      "\t---------------------------------------------------------------------\n",
      ":: retrieving :: org.apache.spark#spark-submit-parent-8ec415c2-5ee3-43dc-9788-b8b1d74aedb9\n",
      "\tconfs: [default]\n",
      "\t1 artifacts copied, 47 already retrieved (25kB/17ms)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22/10/01 21:37:23 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22/10/01 21:37:26 WARN MetricsConfig: Cannot locate configuration: tried hadoop-metrics2-s3a-file-system.properties,hadoop-metrics2.properties\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+-------+-------+-----+-------+-------+-------+-----+------------------+--------+\n",
      "|LatD | \"LatM\"| \"LatS\"| \"NS\"| \"LonD\"| \"LonM\"| \"LonS\"| \"EW\"| \"City\"           | \"State\"|\n",
      "+-----+-------+-------+-----+-------+-------+-------+-----+------------------+--------+\n",
      "|   41|    5  |   59  | \"N\" |     80|   39  |    0  | \"W\" | \"Youngstown\"     | OH     |\n",
      "|   42|   52  |   48  | \"N\" |     97|   23  |   23  | \"W\" | \"Yankton\"        | SD     |\n",
      "|   46|   35  |   59  | \"N\" |    120|   30  |   36  | \"W\" | \"Yakima\"         | WA     |\n",
      "|   42|   16  |   12  | \"N\" |     71|   48  |    0  | \"W\" | \"Worcester\"      | MA     |\n",
      "|   43|   37  |   48  | \"N\" |     89|   46  |   11  | \"W\" | \"Wisconsin Dells\"| WI     |\n",
      "|   36|    5  |   59  | \"N\" |     80|   15  |    0  | \"W\" | \"Winston-Salem\"  | NC     |\n",
      "|   49|   52  |   48  | \"N\" |     97|    9  |    0  | \"W\" | \"Winnipeg\"       | MB     |\n",
      "|   39|   11  |   23  | \"N\" |     78|    9  |   36  | \"W\" | \"Winchester\"     | VA     |\n",
      "|   34|   14  |   24  | \"N\" |     77|   55  |   11  | \"W\" | \"Wilmington\"     | NC     |\n",
      "|   39|   45  |    0  | \"N\" |     75|   33  |    0  | \"W\" | \"Wilmington\"     | DE     |\n",
      "|   48|    9  |    0  | \"N\" |    103|   37  |   12  | \"W\" | \"Williston\"      | ND     |\n",
      "|   41|   15  |    0  | \"N\" |     77|    0  |    0  | \"W\" | \"Williamsport\"   | PA     |\n",
      "|   37|   40  |   48  | \"N\" |     82|   16  |   47  | \"W\" | \"Williamson\"     | WV     |\n",
      "|   33|   54  |    0  | \"N\" |     98|   29  |   23  | \"W\" | \"Wichita Falls\"  | TX     |\n",
      "|   37|   41  |   23  | \"N\" |     97|   20  |   23  | \"W\" | \"Wichita\"        | KS     |\n",
      "|   40|    4  |   11  | \"N\" |     80|   43  |   12  | \"W\" | \"Wheeling\"       | WV     |\n",
      "|   26|   43  |   11  | \"N\" |     80|    3  |    0  | \"W\" | \"West Palm Beach\"| FL     |\n",
      "|   47|   25  |   11  | \"N\" |    120|   19  |   11  | \"W\" | \"Wenatchee\"      | WA     |\n",
      "|   41|   25  |   11  | \"N\" |    122|   23  |   23  | \"W\" | \"Weed\"           | CA     |\n",
      "|   31|   13  |   11  | \"N\" |     82|   20  |   59  | \"W\" | \"Waycross\"       | GA     |\n",
      "+-----+-------+-------+-----+-------+-------+-------+-----+------------------+--------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import sys, os\n",
    "from pyspark.conf import SparkConf\n",
    "from pyspark.sql import SparkSession, Catalog\n",
    "from pyspark.sql import DataFrame, DataFrameStatFunctions, DataFrameNaFunctions\n",
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql import types as T\n",
    "from pyspark.sql.types import Row\n",
    "from subprocess import check_output\n",
    "\n",
    "SPARK_DRIVER_HOST = check_output([\"hostname\", \"-i\"]).decode(encoding=\"utf-8\").strip()\n",
    "spark_conf = SparkConf()\n",
    "spark_conf.setAll([\n",
    "    ('spark.master', 'spark://spark:7077'),\n",
    "    ('spark.app.name', 'myApp'),\n",
    "    ('spark.submit.deployMode', 'client'),\n",
    "    ('spark.ui.showConsoleProgress', 'true'),\n",
    "    ('spark.eventLog.enabled', 'false'),\n",
    "    ('spark.logConf', 'false'),\n",
    "    ('spark.driver.bindAddress', '0.0.0.0'),\n",
    "    ('spark.driver.host', SPARK_DRIVER_HOST),\n",
    "    #works ('spark.jars.packages', 'org.apache.hadoop:hadoop-aws:3.2.0,com.amazonaws:aws-java-sdk-bundle:1.11.704,org.apache.hadoop:hadoop-common:3.2.0,org.apache.spark:spark-hadoop-cloud_2.12:3.3.0'),\n",
    "    ('spark.jars.packages', 'org.apache.hadoop:hadoop-aws:3.2.0,com.amazonaws:aws-java-sdk-bundle:1.11.704,org.apache.spark:spark-hadoop-cloud_2.12:3.3.0'),\n",
    "    (\"spark.hadoop.fs.s3a.endpoint\", 'http://minio:9000'),\n",
    "    ('spark.hadoop.fs.s3a.access.key', 'minio-root-user'),\n",
    "    ('spark.hadoop.fs.s3a.secret.key', 'minio-root-password'),\n",
    "    ('spark.hadoop.fs.s3a.bucket.all.committer.magic.enabled', True),\n",
    "    (\"spark.hadoop.fs.s3a.fast.upload\", True),\n",
    "    (\"spark.hadoop.fs.s3a.path.style.access\", True),\n",
    "    (\"spark.hadoop.fs.s3a.impl\", \"org.apache.hadoop.fs.s3a.S3AFileSystem\")\n",
    "])\n",
    " \n",
    "spark_sess          = SparkSession.builder.config(conf=spark_conf).getOrCreate()\n",
    "spark_ctxt          = spark_sess.sparkContext\n",
    "spark_reader        = spark_sess.read\n",
    "spark_streamReader  = spark_sess.readStream\n",
    "spark_ctxt.setLogLevel(\"WARN\")\n",
    "\n",
    "citiesDF = spark_sess.read.option(\"header\",True).csv('s3a://cities/cities.csv')\n",
    "\n",
    "citiesDF.show(truncate=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "81e3f4e9-0235-48ed-8b4e-a948c9416dcb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['LatD',\n",
       " ' \"LatM\"',\n",
       " ' \"LatS\"',\n",
       " ' \"NS\"',\n",
       " ' \"LonD\"',\n",
       " ' \"LonM\"',\n",
       " ' \"LonS\"',\n",
       " ' \"EW\"',\n",
       " ' \"City\"',\n",
       " ' \"State\"']"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "citiesDF.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "83e7d1b9-6ae7-479d-8a2a-92f81cb575e6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[LatM: string, LonM: string]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cleanCitiesDF = citiesDF \\\n",
    "   .withColumnRenamed(' \"LatM\"', 'LatM') \\\n",
    "   .withColumnRenamed(' \"LonM\"', 'LonM') \\\n",
    "   .select('LatM', 'LonM')\n",
    "cleanCitiesDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "46aa6eea-04fc-4531-aaeb-d126190720df",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+-----+\n",
      "| LatM| LonM|\n",
      "+-----+-----+\n",
      "|    5|   39|\n",
      "|   52|   23|\n",
      "|   35|   30|\n",
      "|   16|   48|\n",
      "|   37|   46|\n",
      "|    5|   15|\n",
      "|   52|    9|\n",
      "|   11|    9|\n",
      "|   14|   55|\n",
      "|   45|   33|\n",
      "|    9|   37|\n",
      "|   15|    0|\n",
      "|   40|   16|\n",
      "|   54|   29|\n",
      "|   41|   20|\n",
      "|    4|   43|\n",
      "|   43|    3|\n",
      "|   25|   19|\n",
      "|   25|   23|\n",
      "|   13|   20|\n",
      "+-----+-----+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "cleanCitiesDF.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3fefb972-35cd-44b6-8947-64c881342c87",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "cleanCitiesDF.write.format(\"csv\").save(\"s3a://cities/cleanCities4.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "61d3162c-9b1d-4837-a1be-c5a8689d16e6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+---+\n",
      "|  5| 39|\n",
      "+---+---+\n",
      "| 52| 23|\n",
      "| 35| 30|\n",
      "| 16| 48|\n",
      "| 37| 46|\n",
      "|  5| 15|\n",
      "| 52|  9|\n",
      "| 11|  9|\n",
      "| 14| 55|\n",
      "| 45| 33|\n",
      "|  9| 37|\n",
      "| 15|  0|\n",
      "| 40| 16|\n",
      "| 54| 29|\n",
      "| 41| 20|\n",
      "|  4| 43|\n",
      "| 43|  3|\n",
      "| 25| 19|\n",
      "| 25| 23|\n",
      "| 13| 20|\n",
      "| 57| 38|\n",
      "+---+---+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "citiesDFRecovered = spark_sess.read.option(\"header\",True).csv('s3a://cities/cleanCities4.csv')\n",
    "citiesDFRecovered.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
